AWSTemplateFormatVersion: "2010-09-09"
Description: Free Tier ETL - S3 + SNS + SQS + Lambdas

Parameters:
  ValidationLambdaRoleArn:
    Type: String
    Description: ARN of the IAM Role for Validation Lambda
  TransformationLambdaRoleArn:
    Type: String
    Description: ARN of the IAM Role for Transformation Lambda
  StepFunctionsRoleArn:
    Type: String
    Description: ARN of the IAM Role for Step Functions
  EventBridgeInvokeStepFunctionsRoleArn:
    Type: String
    Description: ARN of the IAM Role for EventBridge to invoke Step Functions

Resources:

  # --------------------
  # S3 Buckets
  # --------------------

  RawBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: aws-free-tier-etl-raw-446072489762

  CuratedBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: aws-free-tier-etl-curated-446072489762

  # --------------------
  # SNS Topic
  # --------------------

  ETLTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: aws-free-tier-etl-topic

  # --------------------
  # SQS Queues + DLQ
  # --------------------

  ETLDLQ:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: aws-free-tier-etl-dlq

  ETLQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: aws-free-tier-etl-queue
      RedrivePolicy:
        deadLetterTargetArn: !GetAtt ETLDLQ.Arn
        maxReceiveCount: 3

  QueuePolicy:
    Type: AWS::SQS::QueuePolicy
    Properties:
      Queues:
        - !Ref ETLQueue
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal: "*"
            Action: sqs:SendMessage
            Resource: !GetAtt ETLQueue.Arn
            Condition:
              ArnEquals:
                aws:SourceArn: !Ref ETLTopic

  SNSSubscription:
    Type: AWS::SNS::Subscription
    Properties:
      TopicArn: !Ref ETLTopic
      Protocol: sqs
      Endpoint: !GetAtt ETLQueue.Arn

  # --------------------
  # Validation Lambda
  # --------------------

  ValidationLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: aws-free-tier-etl-validation
      Runtime: python3.10
      Handler: index.lambda_handler
      Role: !Ref ValidationLambdaRoleArn
      Timeout: 30
      Environment:
        Variables:
          SNS_TOPIC_ARN: !Ref ETLTopic
      Code:
        ZipFile: |
          import csv
          import io
          import json
          import boto3

          s3 = boto3.client("s3")

          REQUIRED_COLUMNS = ["email", "country"]
          MIN_DATA_ROWS = 1


          def lambda_handler(event, context):
              """
              Validation responsibilities:
              - Header validation (required columns)
              - Row count validation (non-empty file)
              """

              bucket = event["bucket"]
              key = event["key"]

              csv_content = read_csv_from_s3(bucket, key)
              header, rows = parse_csv(csv_content)

              validate_header(header)
              validate_row_count(rows)

              return {
                  "bucket": bucket,
                  "key": key,
                  "valid": True,
                  "metrics": {
                      "total_rows": len(rows),
                      "column_count": len(header)
                  }
              }


          # -----------------------------
          # Helper functions
          # -----------------------------

          def read_csv_from_s3(bucket, key):
              """Read CSV file from S3 bucket."""
              try:
                  obj = s3.get_object(Bucket=bucket, Key=key)
                  return obj["Body"].read().decode("utf-8")
              except Exception as e:
                  raise ValueError(f"Failed to read CSV from S3: {str(e)}")


          def parse_csv(csv_content):
              """Parse CSV content and return header and data rows."""
              reader = csv.reader(io.StringIO(csv_content))
              rows = list(reader)
              
              if not rows:
                  raise ValueError("CSV file is empty")
              
              header = rows[0]
              data_rows = rows[1:]
              
              return header, data_rows


          def validate_header(header):
              """Validate that all required columns are present in the header (case-insensitive)."""
              header_lower = [col.lower().strip() for col in header]
              missing_columns = [col for col in REQUIRED_COLUMNS if col not in header_lower]
              
              if missing_columns:
                  raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")


          def validate_row_count(rows):
              """Validate that the CSV has at least the minimum required data rows."""
              if len(rows) < MIN_DATA_ROWS:
                  raise ValueError(f"CSV must have at least {MIN_DATA_ROWS} data row(s), found {len(rows)}")

  # --------------------
  # Transformation Lambda
  # --------------------

  TransformationLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: aws-free-tier-etl-transform
      Runtime: python3.10
      Handler: index.lambda_handler
      Role: !Ref TransformationLambdaRoleArn
      Timeout: 30
      Layers:
        - arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python310:12
      Code:
        ZipFile: |
          import json
          import boto3
          import pandas as pd
          import io
          import datetime

          s3 = boto3.client("s3")
          CURATED_BUCKET = "aws-free-tier-etl-curated-446072489762"

          def lambda_handler(event, context):
              """
              Transformation responsibilities:
              - Read input data from event
              - Convert to Pandas DataFrame
              - Write to Parquet format
              - Upload to Curated S3 Bucket with timestamp
              """
              try:
                  # 1. Parse Input
                  all_records = []
                  
                  for record in event.get("Records", []):
                      body = json.loads(record["body"])
                      if "record" in body:
                          all_records.append(body["record"])
                      else:
                          all_records.append(body)
                  
                  if not all_records:
                      print("No records to process.")
                      return {"status": "skipped", "reason": "no_records"}

                  # 2. Convert to DataFrame
                  df = pd.DataFrame(all_records)
                  
                  # 3. Convert to Parquet
                  parquet_buffer = io.BytesIO()
                  df.to_parquet(parquet_buffer, index=False)
                  parquet_content = parquet_buffer.getvalue()

                  # 4. Generate Timestamped Key
                  timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                  file_name = f"data_{timestamp}.parquet"
                  
                  # 5. Upload to S3
                  s3.put_object(
                      Bucket=CURATED_BUCKET,
                      Key=file_name,
                      Body=parquet_content,
                      ContentType="application/x-parquet"
                  )
                  
                  print(f"Successfully uploaded {file_name} to {CURATED_BUCKET}")
                  
                  return {
                      "status": "success", 
                      "file": file_name, 
                      "record_count": len(all_records)
                  }

              except Exception as e:
                  print(f"Transformation failed: {str(e)}")
                  raise e

  # --------------------
  # SQS â†’ Lambda Wiring
  # --------------------

  TransformationLambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref TransformationLambda
      Action: lambda:InvokeFunction
      Principal: sqs.amazonaws.com
      SourceArn: !GetAtt ETLQueue.Arn

  SQSToTransformationMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt ETLQueue.Arn
      FunctionName: !Ref TransformationLambda
      BatchSize: 1
      Enabled: true

  # --------------------
  # Step Functions
  # --------------------

  ValidationStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: aws-free-tier-etl-validation-sm
      RoleArn: !Ref StepFunctionsRoleArn
      DefinitionString:
        Fn::Sub: |
          {
            "Comment": "ETL validation workflow",
            "StartAt": "ValidateData",
            "States": {
              "ValidateData": {
                "Type": "Task",
                "Resource": "arn:aws:states:::lambda:invoke",
                "Parameters": {
                  "FunctionName": "${ValidationLambda.Arn}",
                  "Payload.$": "$"
                },
                "Retry": [
                  {
                    "ErrorEquals": ["Lambda.ServiceException", "Lambda.TooManyRequestsException"],
                    "IntervalSeconds": 2,
                    "MaxAttempts": 2,
                    "BackoffRate": 2
                  }
                ],
                "End": true
              }
            }
          }

  # --------------------
  # EventBridge Rule
  # --------------------

  ETLScheduleRule:
    Type: AWS::Events::Rule
    Properties:
      Name: aws-free-tier-etl-schedule
      ScheduleExpression: rate(1 day)
      State: ENABLED
      Targets:
        - Arn: !Ref ValidationStateMachine
          Id: StepFunctionsTarget
          RoleArn: !Ref EventBridgeInvokeStepFunctionsRoleArn
          Input: |
            {
              "record": {
                "id": "scheduled-run",
                "source": "eventbridge"
              }
            }
